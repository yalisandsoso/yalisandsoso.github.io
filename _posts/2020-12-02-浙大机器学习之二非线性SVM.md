---
bg: "tools.jpg"
layout: post
title:  浙大机器学习之二非线性SVM
crawlertitle: 机器学习
summary: SVM
date:   2020-12-02
categories: 机器学习
tags: ['Gengzi 12']
author: hekun
bg: "african-penguins.jpg"
---

 浙大机器学习之二非线性模型-SVM。

## Overview

```mathematica
SVM 处理非线性（加正则项，改变限制条件）
1. 最小化 1/2*||W||^2 + C*累加(esi)  i=1~N,   esi->Slack Variable 松弛变量     
2. 限制条件:   (1)  Yi*[WTXi + b] >= 1 - esi (i=1~N)
             (2)   esi >= 0
(W,b,esi)
C*cl(esi) i=1~N  -> regulation Term 正则项
C 事先设定的参数 

需要讲清楚为何需要上述的处理
```

### Theory

```mathematica
1. 低纬到高维
Fai(X)
X ---》 Fai(X)
低纬    高纬

证明在“低纬”线性不可分的数据集在”高纬“线性可分的概率更大！！！！！
WT*Fai(X)+b = 0

X1 =[0 0] class 1
X2 =[1 1] class 0
x3 =[0 1] class 0
x4 =[1 1] class 1

基于一个实验 在低纬和高纬同时做一件事
在空间中随机的分布不同类别标签
在高纬更大概率是可分的，当维度数接近无限的时候
可分的概率将接近或者等于1

如何选取Fai(X)?
<1>. Fai(X)是无限维

如果Fai(X)是无限维，这样就遇到一个问题，就是WT待定参数也将变成无限维度，那么这么解决呢？

事实：
我们可以不知道无限维映射Fai(X)的显式表达，我们只需要知道一个核函数(Kernel Function)
K(X1,X2) = Fai(X1)T*Fai(X2)
则这个优化问题仍然可解
Fai(X1)T*Fai(X2) ===> Fai(X1)、Fai(X2)两个无限维的内积，这是一个数

Kenel Function:
1. K(X1, X2) = e^(-(||X1-X2||^2)/2*deta^2) （高斯核） = Fai(X1)T*Fai(X2)
2. K(X1, X2) = (X1T*X2+1)^d ====> d是多项式的阶数,此时 Fai(X)是有限维

重点： 如何利用K替换Fai(X)
K有哪些条件？  需要泛函分析理论

Mercer's theorem !!!
<a> K（X1，X2）=K（X2，X1）
<b> 对于任意Ci, Xi(i=1~N),有：  （半正定性）
   累加-累加CiCjK(Xi,Xj) >= 0
```

```mathematica
利用K做SVM非线性优化问题

补充知识：（优化理论）

原问题：
Prime Problem- generic
最小化: f(W)
限制条件: 基于gi(W) <= 0 (i=1~K)
            hi(W) = 0 (i=1~M)

对偶问题：
Dual Problem
定义：L（W，arf,omig）= f(W) + 累加（arfi*gi(w), i=1!K）+累加(omigi*hi(w), i=1~M)
                    = f(W) + arfiT*g(W) + omigT*h(W)
最大化： O(arf,omig) = inf{L(w,arf,omig)} 所有的w
限制条件: arfi >=0 (i=1~K)

inf 求最小值
在限制arf,omig的情况下 遍历所有的w求最小值

定理：如果W*是原问题的解，而arf*,omig*是对偶问题的解，则有：
f(W*) >= O(arf*,omig*)

O(arf*,omig*)=inf{L（w,arf*,omig*）}
<= L(w*,arf*,omig*)

===>
G = f(w*) - O(arf*,omig*) >= 0
G叫做原问题与对偶问题的间距(Duality Gap)

（对于某些优化问题，可以证明G=0）！！！

强对偶定理：
  若f(W)为凸函数
```

## Codes

```c++

```

### Other Support

<<Convex optimization>> Stephen Boyd

<<Non-linear Programming>> XILA 
